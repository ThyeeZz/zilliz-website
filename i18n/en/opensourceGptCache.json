{
  "head": {
    "title": "What is GPTCache?",
    "desc": "GPTCache is an open-source tool designed to improve the efficiency and speed of GPT-based applications by implementing a cache to store the responses generated by language models. GPTCache allows users to customize the cache according to their needs, including options for embedding functions, similarity evaluation functions, storage location and eviction. In addition, GPTCache currently supports the OpenAI ChatGPT interface and the Langchain interface.",
    "btnLabel": "Try Zilliz Cloud for Free",
    "getGptCache": "Get GPTCache on GitHub"
  },

  "community": {
    "title": "Built on a strong and growing community.",
    "github": {
      "desc": "GitHub stars"
    }
  },
  "why": {
    "title": "Why GPTCache?",
    "desc1": "Developing a <0>semantic cache</0>, such as GPTCache, to store Large Language Model (LLM) responses can provide several advantages, such as:"
  },
  "features": {
    "conclusion": "Overall, developing a semantic cache for storing LLM responses can offer various benefits, including improved performance, reduced expenses, better scalability, customization, and reduced network latency.",
    "performance": {
      "title": "Improved performance",
      "desc": "Storing LLM responses in a cache can significantly reduce the time it takes to retrieve the response, especially when it has been previously requested and is already present in the cache. Storing responses in a cache can improve the overall performance of your application."
    },
    "expenses": {
      "title": "Reduced expenses",
      "desc": "Most LLM services charge fees based on a combination of the number of requests and <0>token count</0>. Caching LLM responses can reduce the number of API calls made to the service, translating into cost savings. Caching is particularly relevant when dealing with high traffic levels, where API call expenses can be substantial."
    },
    "scalability": {
      "title": "Better scalability",
      "desc": "Caching LLM responses can improve the scalability of your application by reducing the load on the LLM service. Caching helps avoid bottlenecks and ensures that the application can handle a growing number of requests."
    },
    "cost": {
      "title": "Minimize Development Cost",
      "desc": "A semantic cache can be a valuable tool to help reduce costs during the development phase of an LLM (Language Model) app. An LLM application requires an LLM APIs connection even during development, which could become costly. GPTCache offers the same interface as LLM APIs and can store LLM-generated or mocked-up data. GPTCache helps verify your application's features without connecting to the LLM APIs or the network."
    },
    "network": {
      "title": "Reducing network latency",
      "desc": "A semantic cache located closer to the user, reducing the time it takes to retrieve data from the LLM service. By reducing network latency, you can improve the overall user experience."
    },
    "availability": {
      "title": "Improved scalability and availability",
      "desc": "LLM services frequently enforce rate limits, which are constraints that APIs place on the number of times a user or client can access the server within a given timeframe. Hitting a rate limit means that additional requests will be blocked until a certain period has elapsed, leading to a service outage. With GPTCache, you can quickly scale to accommodate an increasing volume of queries, ensuring consistent performance as your application's user base expands."
    }
  },
  "mechanism": {
    "title": "How GPTCache works",
    "desc": [
      "GPTCache takes advantage of data locality in online services by storing commonly accessed data, reducing retrieval time, and easing the backend server load. Unlike traditional cache systems, GPTCache uses semantic caching, identifying and storing similar or related queries to improve cache hit rates.",
      "Using embedding algorithms, GPTCache converts queries into embeddings and employs a vector store for similarity search, enabling retrieval of related queries from the cache. The modular design of GPTCache allows users to customize their semantic cache with various implementations for each module.",
      "While semantic caching may result in false positives and negatives, GPTCache offers three performance metrics to help developers optimize their caching systems.",
      "This process allows GPTCache to identify and retrieve similar or related queries from the cache storage, as illustrated in the diagram below."
    ]
  },
  "keyResources": {
    "title": "Key Resources",
    "resources": [
      {
        "tagLine": "Blog",
        "title": "Intro to OSS Chat",
        "btnLabel": "Read More",
        "desc": "How this app demonstrates the new AI Stack ChatGPT + Vector database + prompt-as-code",
        "link": "/blog/ChatGPT-VectorDB-Prompt-as-code"
      },
      {
        "tagLine": "Blog",
        "title": "Introducing GPTCache",
        "desc": "Improve the efficiency and speed of GPT-based applications by implementing a cache",
        "btnLabel": "Read Blog",
        "link": "/blog/Caching-LLM-Queries-for-performance-improvements"
      },
      {
        "tagLine": "Blog",
        "title": "Yet another cache, but for ChatGPT",
        "desc": "How we built GPTCache in less than 2 weeks and open sourced it",
        "btnLabel": "Read Blog",
        "link": "/blog/Yet-another-cache-but-for-ChatGPT"
      }
    ]
  }
}
